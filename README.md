# Otus-linux Project
## Выпускной проект курса Администратор linux Какунина Дмитрия
### Тема: Кластеризация Jira

__Проект построен на основе полученных во время обучения знаний__

__За основу взята официальная документация [https://developer.atlassian.com/server/jira/platform/developing-for-high-availability-and-clustering/](https://developer.atlassian.com/server/jira/platform/developing-for-high-availability-and-clustering/)__

__Концепция построения: Отказоустойчивость и высокая доступность сисемы__

### Схема кластера

![img](https://github.com/kakunindima/otus_linux_project/blob/master/img/project_2.png)

__Кластер состоит из 13 нод которые можно сгруппировать по типу__

### Jiralb1,2

__ Ноды на которых крутится loadbalancer на основе apache в docker, между нодами настроен keepalived и плавающий ip 10.0.0.10 который является единой точкой входа в для веб, каждый балансер в свою очередь смотрит на обе машины с запущенной jira__

### Jira1,2

__Ноды на которых в docker запущена jira, образ взят из официального docker репозитория [atlassian/jira](hub.docker.com/r/atlassian/jira-software) версии 8.4.0. конфигурация контейнера настроена через docker-compose и запущена внутри ноды как systemd service. Коннект к БД осуществляется на единый ip адресс 10.0.0.20, к каждая нода монтируется к storage также через единый ip адресс 10.0.0.30, сюда мапятся директории для контейнеров с jira - jira-home-node1, jira-home-node2, jira-home-shared.__

### Stor1,2

__Ноды на которых развернуто хранилище на основе glusterfs, создан один replicated volume, между нодами настроен keepalived and float ip 10.0.0.30, клиенты (jira1,2) монтируют шаредный раздел к этому ip (через dns запись stor) что обеспечивает сохранение точк монтирования при отключении одной из нод хранилища__

### Consul1,2,3

__Ноды выполняющие несколько функций:__

__На всех 3х нодах развернут consul агент обеспечивающий кворум для управления кластером patroni__

__На нодах 1 & 2 развернуто: keepalived and float ip 10.0.0.20 обеспечивающий единую точку коннекта к БД. pgbouncer отвечающий за пулл коннектов к мастеру бд, направляющий запросы через развернутый haproxy к мастеру кластера путем проверки состояния patroni (запрос по порту 8008 к каждой ноде бд выдает состояние - мастер 200 или слейв 503).__

### Db1,2,3

__Ноды составляющие кластер postgres на основе patroni. мастер определяется кворумом consul, коннект к базе осуществляется pgbouncer через haproxy.__

### Bml - backup, monitoring, log

__Нода предназначеная для выполнения 3 функций: __

__Единая точка сбора логов на основе journald__

__Выполнения бэкапов (не готово) __

__Обеспечение мониторинга (не готово)__